{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (K-NN) Algorithm\n",
    "The K-Nearest Neighbors (K-NN) algorithm is a simple, supervised machine learning algorithm that can be used for both classification and regression tasks. It operates on the principle that similar data points are close to each other in the feature space.\n",
    "\n",
    "### How K-NN Works:\n",
    "1. **Choose the number of neighbors (K):** Select the number of nearest neighbors to consider when making predictions.\n",
    "2. **Calculate Distance:** Compute the distance between the new data point and all the training data points. Common distance metrics include Euclidean, Manhattan, and Minkowski distances.\n",
    "3. **Identify Neighbors:** Identify the K training data points that are closest to the new data point.\n",
    "4. **Make Predictions:**\n",
    "    - **For Classification:** The new data point is assigned the class that is most common among its K nearest neighbors.\n",
    "    - **For Regression:** The prediction is the average of the values of its K nearest neighbors.\n",
    "\n",
    "### Advantages:\n",
    "- Simple and easy to implement.\n",
    "- No assumptions about the data distribution.\n",
    "- Effective with a small number of features.\n",
    "\n",
    "### Disadvantages:\n",
    "- Computationally expensive, especially with large datasets.\n",
    "- Sensitive to the choice of K and the distance metric.\n",
    "- Performance can degrade with high-dimensional data (curse of dimensionality).\n",
    "\n",
    "### Applications:\n",
    "- Pattern recognition\n",
    "- Image and video recognition\n",
    "- Recommendation systems\n",
    "- Anomaly detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbor (KNN) Algorithm for Machine Learning\n",
    "\n",
    "K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on **Supervised Learning** technique.\n",
    "\n",
    "## How K-NN Works\n",
    "\n",
    "- K-NN algorithm assumes the similarity between the new case/data and available cases and puts the new case into the category that is most similar to the available categories.\n",
    "- K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears, it can be easily classified into a well-suited category using the K-NN algorithm.\n",
    "- K-NN algorithm can be used for **Regression** as well as for **Classification**, but mostly it is used for **Classification** problems.\n",
    "\n",
    "## Key Characteristics of K-NN\n",
    "\n",
    "- **Non-parametric algorithm**: K-NN does not make any assumptions about the underlying data.\n",
    "- **Lazy learner algorithm**: K-NN does not learn from the training set immediately. Instead, it stores the dataset and performs classification only when new data is provided.\n",
    "\n",
    "## Example\n",
    "\n",
    "Suppose we have an image of a creature that looks similar to a cat and a dog, but we want to know whether it is a cat or a dog. For this identification, we can use the K-NN algorithm, as it works on a similarity measure. \n",
    "\n",
    "- Our K-NN model will find the similar features of the new dataset to the cat and dog images.\n",
    "- Based on the most similar features, it will classify the image into either the cat or dog category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://github.com/user-attachments/assets/6bacac03-82ec-4347-89e4-e751441d42f7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://github.com/user-attachments/assets/bc03c4fa-460b-4fc7-933b-e9f7c13553b9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://github.com/user-attachments/assets/3cc16ce2-6553-4dad-a774-f6cec546bcab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to choose the value of k for KNN Algorithm?\n",
    "If the dataset has significant outliers or noise a higher k can help smooth out the predictions and reduce the influence of noisy data. However choosing very high value can lead to underfitting where the model becomes too simplistic.<br>\n",
    "Statistical Methods for Selecting k:<br>\n",
    "\n",
    "- Cross-Validation: A robust method for selecting the best k is to perform k-fold cross-validation. This involves splitting the data into k subsets training the model on some subsets and testing it on the remaining ones and repeating this for each subset. The value of k that results in the highest average validation accuracy is usually the best choice.\n",
    "- Elbow Method: In the elbow method we plot the model’s error rate or accuracy for different values of k. As we increase k the error usually decreases initially. However after a certain point the error rate starts to decrease more slowly. This point where the curve forms an “elbow” that point is considered as best k.\n",
    "- Odd Values for k: It’s also recommended to choose an odd value for k especially in classification tasks to avoid ties when deciding the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Metrics Used in KNN Algorithm\n",
    "# Distance Metrics in KNN\n",
    "\n",
    "There are several distance metrics that can be used in the KNN algorithm to measure the similarity between data points. Some of the most commonly used distance metrics are:\n",
    "\n",
    "## 1. Euclidean Distance\n",
    "The most common distance metric, calculated as the straight-line distance between two points in Euclidean space.\n",
    "The Euclidean distance between two points \\( \\mathbf{p} \\) and \\( \\mathbf{q} \\) in Euclidean space is given by:\n",
    "\n",
    "$$\n",
    "d(\\mathbf{p}, \\mathbf{q}) = \\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2}\n",
    "$$\n",
    "\n",
    "## 2. Manhattan Distance\n",
    "Also known as L1 distance or city block distance, it is the sum of the absolute differences between the coordinates of the points.\n",
    "The Manhattan distance between two points \\( \\mathbf{p} \\) and \\( \\mathbf{q} \\) is given by:\n",
    "\n",
    "$$\n",
    "d(\\mathbf{p}, \\mathbf{q}) = \\sum_{i=1}^{n} |p_i - q_i|\n",
    "$$\n",
    "\n",
    "## 3. Minkowski Distance\n",
    "A generalization of both Euclidean and Manhattan distances, defined by a parameter \\( p \\). When \\( p=1 \\), it is equivalent to Manhattan distance, and when \\( p=2 \\), it is equivalent to Euclidean distance.\n",
    "The Minkowski distance between two points \\( \\mathbf{p} \\) and \\( \\mathbf{q} \\) is given by:\n",
    "\n",
    "$$\n",
    "d(\\mathbf{p}, \\mathbf{q}) = \\left( \\sum_{i=1}^{n} |p_i - q_i|^p \\right)^{\\frac{1}{p}}\n",
    "$$\n",
    "\n",
    "When \\( p=1 \\), it is equivalent to Manhattan distance, and when \\( p=2 \\), it is equivalent to Euclidean distance.\n",
    "\n",
    "## 4. Hamming Distance\n",
    "Used for categorical data, it measures the number of positions at which the corresponding elements are different.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://github.com/user-attachments/assets/e39285cc-b700-41d8-a4af-36e5edc2a9e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step-by-Step Explanation of KNN\n",
    "Initialization:\n",
    "\n",
    "Choose the number of neighbors K. This is a hyperparameter that you need to decide before running the algorithm.\n",
    "Calculate Distance:\n",
    "\n",
    "For a given data point that you want to classify, calculate the distance between this point and all other points in the dataset. Common distance metrics include Euclidean distance, Manhattan distance, and Minkowski distance.\n",
    "Sort Distances:\n",
    "\n",
    "Sort the calculated distances in ascending order. This helps in identifying the nearest neighbors.\n",
    "Select Neighbors:\n",
    "\n",
    "Select the top K nearest neighbors from the sorted list. These are the K data points that are closest to the given data point.\n",
    "Voting (for Classification):\n",
    "\n",
    "For classification tasks, each of the K neighbors will vote for their class. The class with the most votes is assigned to the given data point.\n",
    "Averaging (for Regression):\n",
    "\n",
    "For regression tasks, the average of the values of the K nearest neighbors is taken as the predicted value for the given data point.\n",
    "Output:\n",
    "\n",
    "The algorithm outputs the predicted class (for classification) or the predicted value (for regression) for the given data point.\n",
    "Example\n",
    "Let's consider a simple example to illustrate KNN for a classification task:\n",
    "\n",
    "Dataset:\n",
    "\n",
    "Suppose we have a dataset with two features (x1, x2) and two classes (A, B).\n",
    "New Data Point:\n",
    "\n",
    "We have a new data point (x1=3, x2=4) that we want to classify.\n",
    "Calculate Distance:\n",
    "\n",
    "Calculate the Euclidean distance between the new data point and all points in the dataset.\n",
    "Sort Distances:\n",
    "\n",
    "Sort these distances to find the nearest neighbors.\n",
    "Select Neighbors:\n",
    "\n",
    "Choose the top K nearest neighbors (let's say K=3).\n",
    "Voting:\n",
    "\n",
    "The selected neighbors vote for their class. Suppose the votes are: 2 votes for class A and 1 vote for class B.\n",
    "Output:\n",
    "\n",
    "The new data point is classified as class A since it received the majority of the votes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
