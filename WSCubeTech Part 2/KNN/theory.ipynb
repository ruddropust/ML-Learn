{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (K-NN) Algorithm\n",
    "The K-Nearest Neighbors (K-NN) algorithm is a simple, supervised machine learning algorithm that can be used for both classification and regression tasks. It operates on the principle that similar data points are close to each other in the feature space.\n",
    "\n",
    "### How K-NN Works:\n",
    "1. **Choose the number of neighbors (K):** Select the number of nearest neighbors to consider when making predictions.\n",
    "2. **Calculate Distance:** Compute the distance between the new data point and all the training data points. Common distance metrics include Euclidean, Manhattan, and Minkowski distances.\n",
    "3. **Identify Neighbors:** Identify the K training data points that are closest to the new data point.\n",
    "4. **Make Predictions:**\n",
    "    - **For Classification:** The new data point is assigned the class that is most common among its K nearest neighbors.\n",
    "    - **For Regression:** The prediction is the average of the values of its K nearest neighbors.\n",
    "\n",
    "### Advantages:\n",
    "- Simple and easy to implement.\n",
    "- No assumptions about the data distribution.\n",
    "- Effective with a small number of features.\n",
    "\n",
    "### Disadvantages:\n",
    "- Computationally expensive, especially with large datasets.\n",
    "- Sensitive to the choice of K and the distance metric.\n",
    "- Performance can degrade with high-dimensional data (curse of dimensionality).\n",
    "\n",
    "### Applications:\n",
    "- Pattern recognition\n",
    "- Image and video recognition\n",
    "- Recommendation systems\n",
    "- Anomaly detection\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
