{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is a table used to evaluate the performance of a classification algorithm. It compares the actual target values with those predicted by the model. The matrix has four main components:\n",
    "\n",
    "- **True Positive (TP)**: The number of correct positive predictions.\n",
    "- **True Negative (TN)**: The number of correct negative predictions.\n",
    "- **False Positive (FP)**: The number of incorrect positive predictions (Type I error). actual prediction is negetive, but model predict positive\n",
    "- **False Negative (FN)**: The number of incorrect negative predictions (Type II error). actual prediction is positive, but model predict negetive\n",
    "\n",
    "The confusion matrix helps in understanding the types of errors made by the classifier and is useful for calculating various performance metrics such as accuracy, precision, recall, and F1-score.\n",
    "\n",
    "Example of a confusion matrix:\n",
    "\n",
    "| Actual \\ Predicted | Positive | Negative |\n",
    "|--------------------|----------|----------|\n",
    "| Positive           | TP       | FN       |\n",
    "| Negative           | FP       | TN       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://github.com/user-attachments/assets/1bd6ade4-a0b1-40af-9fed-87fe352751b9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://github.com/user-attachments/assets/56a697a0-5275-427c-9b3d-661ee6b6df80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://github.com/user-attachments/assets/d6a03378-549c-48a0-8635-3efec5992ff4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://github.com/user-attachments/assets/9066b495-1fe8-4f23-b897-0b1815bc3472)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity, Precision, Recall, F1-Score\n",
    "\n",
    "- **Sensitivity (Recall)**: Sensitivity, also known as recall or true positive rate, measures the proportion of actual positives that are correctly identified by the model. It is calculated as:\n",
    "\n",
    "  $$\n",
    "  \\text{Sensitivity} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "  $$\n",
    "\n",
    "- **Precision**: Precision, also known as positive predictive value, measures the proportion of positive predictions that are actually correct. It is calculated as:\n",
    "\n",
    "  $$\n",
    "  \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "  $$\n",
    "\n",
    "- **Recall**: Recall is another term for sensitivity, as defined above.\n",
    "\n",
    "- **F1-Score**: The F1-score is the harmonic mean of precision and recall. It provides a single metric that balances both the precision and recall of the model. It is calculated as:\n",
    "\n",
    "  $$\n",
    "  \\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "  $$\n",
    "\n",
    "These metrics are essential for evaluating the performance of classification models, especially when dealing with imbalanced datasets.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
