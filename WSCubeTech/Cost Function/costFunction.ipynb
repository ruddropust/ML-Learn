{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``A cost function,``** also known as a loss function or objective function, is a mathematical function that measures the error or difference between the predicted values and the actual values in a machine learning model. The goal of training a model is to minimize this cost function, thereby improving the accuracy of the model's predictions. Common cost functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks.\n",
    "\n",
    "**``A loss function,``** also known as a cost function or objective function, is a mathematical function used in machine learning to measure the error or difference between the predicted values and the actual values. The primary goal of training a machine learning model is to minimize this loss function, which in turn improves the accuracy of the model's predictions.\n",
    "\n",
    "Here are two common types of loss functions:\n",
    "\n",
    "**Mean Squared Error (MSE):** Used for regression tasks, it calculates the average of the squared differences between the predicted and actual values.</br>\n",
    "**Cross-Entropy Loss:** Used for classification tasks, it measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
    "Minimizing the loss function helps in optimizing the model parameters to make better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Regression Cost Function\n",
    "\n",
    "A **regression cost function** is a mathematical function used to measure the error or difference between the predicted values and the actual values in a regression model. The primary goal of training a regression model is to minimize this cost function, thereby improving the accuracy of the model's predictions.\n",
    "\n",
    "#### Types of Regression Cost Functions\n",
    "\n",
    "# Regression Error Metrics\n",
    "\n",
    "1. **Mean Squared Error (MSE)**\n",
    "- **Definition:** MSE calculates the average of the squared differences between the predicted values and the actual values.also known as ``L2 Loss``\n",
    "- **Formula:**  \n",
    "  $$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "  where $ y_i $ is the actual value, $ \\hat{y}_i $ is the predicted value, and $ n $ is the number of data points.\n",
    "- **Usage:** MSE is commonly used in regression tasks because it penalizes larger errors more than smaller ones, making it sensitive to outliers.\n",
    "\n",
    " 2.**Mean Absolute Error (MAE)**\n",
    "- **Definition:** MAE calculates the average of the absolute differences between the predicted values and the actual values.\n",
    "- **Formula:**  \n",
    "  $$ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$\n",
    "  where $ y_i $ is the actual value, $ \\hat{y}_i $ is the predicted value, and $ n $ is the number of data points.\n",
    "- **Usage:** MAE is less sensitive to outliers compared to MSE and provides a more interpretable measure of average error.\n",
    "\n",
    " 3. **Root Mean Squared Error (RMSE)**\n",
    "- **Definition:** RMSE is the square root of the average of the squared differences between the predicted values and the actual values.\n",
    "- **Formula:**  \n",
    "  $$ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $$\n",
    "  where $ y_i $ is the actual value, $ \\hat{y}_i $ is the predicted value, and $ n $ is the number of data points.\n",
    "- **Usage:** RMSE is useful when you want to penalize larger errors more heavily and is often used in conjunction with MSE.\n",
    "\n",
    "\n",
    "4. **Huber Loss:**\n",
    "    - **Definition:** Huber Loss is a combination of MSE and MAE, which is less sensitive to outliers than MSE and more robust than MAE.\n",
    "    - **Formula:** \n",
    "      \\[\n",
    "      L_\\delta(y, \\hat{y}) = \n",
    "      \\begin{cases} \n",
    "      \\frac{1}{2}(y - \\hat{y})^2 & \\text{for } |y - \\hat{y}| \\leq \\delta \\\\\n",
    "      \\delta |y - \\hat{y}| - \\frac{1}{2}\\delta^2 & \\text{otherwise}\n",
    "      \\end{cases}\n",
    "      \\]\n",
    "      where \\( y \\) is the actual value, \\( \\hat{y} \\) is the predicted value, and \\( \\delta \\) is a threshold parameter.\n",
    "    - **Usage:** Huber Loss is used when you want a balance between MSE and MAE, providing robustness to outliers while maintaining sensitivity to small errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mean Squared Error (MSE)**\n",
    "\n",
    "![Image](https://github.com/user-attachments/assets/870c5c2c-0194-4cfe-8cf3-b3bdab5ef3e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Minimize Mean Squared Error in Model Training?\n",
    "To minimize Mean Squared Error during the model training several strategies can be employed including:</br>\n",
    "\n",
    "* **Feature selection:** Choosing relevant features that contribute most to the reducing prediction errors.\n",
    "* **Model selection:** The Experimenting with the different algorithms and model architectures to the identify the best-performing model.\n",
    "* **Hyperparameter tuning:** The Optimizing model hyperparameters such as the learning rate, regularization strength and network depth to the improve predictive accuracy.\n",
    "\n",
    "\n",
    "<button height=\"50px\" width=\"150px\" color=\"red\"><a href=\"https://observablehq.com/@yizhe-ang/interactive-visualization-of-linear-regression\">Visualization</a></button>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Limitations of Mean Squared Error\n",
    "The advantages and limitations of mean squared error is mentioned below:\n",
    "\n",
    "### Advantages\n",
    "- Provides the comprehensive measure of the model accuracy.\n",
    "- Sensitive to the both large and small errors.\n",
    "- Easy to the calculate and interpret.\n",
    "### Limitations\n",
    "- It can be heavily influenced by the outliers.\n",
    "- It penalizes large errors disproportionately which may not always be desirable.\n",
    "\n",
    "**Resources**</br><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/LvC68w9JS4Y?si=u_yB3hwc0gwtfBRU\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe></br>\n",
    "**``06:02:35 - Regression Cost Function - R Squared score & Adjusted R Squared Regression Analysis``**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Mean Absolute Error (MAE)?\n",
    "Mean Absolute Error calculates the average ``difference between the calculated values and actual values``. It is also known as scale-dependent accuracy as it calculates error in observations taken on the same scale used to predict the accuracy of the machine learning model.<br>\n",
    "- **Formula:**  <br>\n",
    "  $$ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$\n",
    "  where $ y_i $ is the actual value, $ \\hat{y}_i $ is the predicted value, and $ n $ is the number of data points.\n",
    "\n",
    "### Method : Calculating MAE Using sklearn.metrics <br>\n",
    "The sklearn.metrics module in Python provides various tools to evaluate the performance of machine learning models. One of the methods available is mean_absolute_error(), which simplifies the calculation of MAE by handling all the necessary steps internally. This method ensures accuracy and efficiency, especially when working with large datasets.<br>\n",
    "<br>\n",
    "**``mean_absolute_error(actual,calculated)``** <br>\n",
    "## Why to Choose Mean Absolute Error?\n",
    "Mean Absolute Error (MAE) is a important metric to assess the performance of regression models. Several key reasons why MAE is significant and widely adopted:<br>\n",
    "\n",
    "**``Interpretability:``** MAE provides a straightforward measure of average error, calculating the average absolute difference between predicted and actual values. For example, an MAE of 5 indicates that predictions, on average, deviate by 5 units from true values.<br>\n",
    "**``Robustness to Outliers:``** MAE is resilient to outliers, treating all errors equally. Unlike Mean Squared Error (MSE), which penalizes larger discrepancies more heavily, MAE maintains stability and reliability when datasets include outliers.<br>\n",
    "**``Practical Application:``** MAE is useful in real-world scenarios where error costs are linear, such as demand forecasting or housing price estimation. It offers insights into average error sizes, making it a preferred metric for evaluating prediction quality based on absolute rather than relative error.<br>\n",
    "**``Linear Penalty:``** The linear nature of MAE means that each errorâ€™s impact on model learning is directly proportional to its magnitude. This allows the model to minimize average errors without disproportionately focusing on larger discrepancies, ensuring balanced performance across all data points.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Root Mean square Error\n",
    "\n",
    "![Image](https://github.com/user-attachments/assets/62740c91-9738-4fc4-935b-5fb5653a2532)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Why Use Root Mean Square Error (RMSE)?**\n",
    "\n",
    "RMSE is preferred over other metrics like Mean Absolute Error (MAE) because it penalizes larger errors more significantly. This makes it sensitive to outliers, which can be beneficial when large errors are particularly undesirable.\n",
    "\n",
    "## **Key Advantages of RMSE**\n",
    "\n",
    "### **1. Intuitive Interpretation**  \n",
    "RMSE quantifies the average magnitude of errors in the same units as the target variable, making it easy to understand how far predictions deviate from actual values.\n",
    "\n",
    "### **2. Sensitivity to Large Errors**  \n",
    "By squaring individual errors, RMSE emphasizes larger discrepancies, helping to identify significant prediction errors that may need attention.\n",
    "\n",
    "### **3. Scale Consistency**  \n",
    "RMSE is expressed in the same units as the predicted values, allowing for straightforward interpretation in practical contexts.\n",
    "\n",
    "### **4. Benchmarking and Comparison**  \n",
    "It serves as a reliable benchmark for comparing different models; lower RMSE values indicate better predictive performance.\n",
    "\n",
    "### **5. Standardization in Reporting**  \n",
    "As a widely accepted metric, RMSE facilitates consistent reporting and communication of model performance across various fields.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**``from sklearn.metrics import mean_squared_error``**<br>\n",
    "**``mse = mean_squared_error(y_true, y_pred)``**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
