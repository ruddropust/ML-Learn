{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``A cost function,``** also known as a loss function or objective function, is a mathematical function that measures the error or difference between the predicted values and the actual values in a machine learning model. The goal of training a model is to minimize this cost function, thereby improving the accuracy of the model's predictions. Common cost functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks.\n",
    "\n",
    "**``A loss function,``** also known as a cost function or objective function, is a mathematical function used in machine learning to measure the error or difference between the predicted values and the actual values. The primary goal of training a machine learning model is to minimize this loss function, which in turn improves the accuracy of the model's predictions.\n",
    "\n",
    "Here are two common types of loss functions:\n",
    "\n",
    "**Mean Squared Error (MSE):** Used for regression tasks, it calculates the average of the squared differences between the predicted and actual values.</br>\n",
    "**Cross-Entropy Loss:** Used for classification tasks, it measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
    "Minimizing the loss function helps in optimizing the model parameters to make better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Regression Cost Function\n",
    "\n",
    "A **regression cost function** is a mathematical function used to measure the error or difference between the predicted values and the actual values in a regression model. The primary goal of training a regression model is to minimize this cost function, thereby improving the accuracy of the model's predictions.\n",
    "\n",
    "#### Types of Regression Cost Functions\n",
    "\n",
    "# Regression Error Metrics\n",
    "\n",
    "1. **Mean Squared Error (MSE)**\n",
    "- **Definition:** MSE calculates the average of the squared differences between the predicted values and the actual values.also known as ``L2 Loss``\n",
    "- **Formula:**  \n",
    "  $$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "  where $ y_i $ is the actual value, $ \\hat{y}_i $ is the predicted value, and $ n $ is the number of data points.\n",
    "- **Usage:** MSE is commonly used in regression tasks because it penalizes larger errors more than smaller ones, making it sensitive to outliers.\n",
    "\n",
    " 2.**Mean Absolute Error (MAE)**\n",
    "- **Definition:** MAE calculates the average of the absolute differences between the predicted values and the actual values.\n",
    "- **Formula:**  \n",
    "  $$ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$\n",
    "  where $ y_i $ is the actual value, $ \\hat{y}_i $ is the predicted value, and $ n $ is the number of data points.\n",
    "- **Usage:** MAE is less sensitive to outliers compared to MSE and provides a more interpretable measure of average error.\n",
    "\n",
    " 3. **Root Mean Squared Error (RMSE)**\n",
    "- **Definition:** RMSE is the square root of the average of the squared differences between the predicted values and the actual values.\n",
    "- **Formula:**  \n",
    "  $$ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $$\n",
    "  where $ y_i $ is the actual value, $ \\hat{y}_i $ is the predicted value, and $ n $ is the number of data points.\n",
    "- **Usage:** RMSE is useful when you want to penalize larger errors more heavily and is often used in conjunction with MSE.\n",
    "\n",
    "\n",
    "4. **Huber Loss:**\n",
    "    - **Definition:** Huber Loss is a combination of MSE and MAE, which is less sensitive to outliers than MSE and more robust than MAE.\n",
    "    - **Formula:** \n",
    "      \\[\n",
    "      L_\\delta(y, \\hat{y}) = \n",
    "      \\begin{cases} \n",
    "      \\frac{1}{2}(y - \\hat{y})^2 & \\text{for } |y - \\hat{y}| \\leq \\delta \\\\\n",
    "      \\delta |y - \\hat{y}| - \\frac{1}{2}\\delta^2 & \\text{otherwise}\n",
    "      \\end{cases}\n",
    "      \\]\n",
    "      where \\( y \\) is the actual value, \\( \\hat{y} \\) is the predicted value, and \\( \\delta \\) is a threshold parameter.\n",
    "    - **Usage:** Huber Loss is used when you want a balance between MSE and MAE, providing robustness to outliers while maintaining sensitivity to small errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mean Squared Error (MSE)**\n",
    "\n",
    "![Image](https://github.com/user-attachments/assets/870c5c2c-0194-4cfe-8cf3-b3bdab5ef3e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Minimize Mean Squared Error in Model Training?\n",
    "To minimize Mean Squared Error during the model training several strategies can be employed including:</br>\n",
    "\n",
    "* **Feature selection:** Choosing relevant features that contribute most to the reducing prediction errors.\n",
    "* **Model selection:** The Experimenting with the different algorithms and model architectures to the identify the best-performing model.\n",
    "* **Hyperparameter tuning:** The Optimizing model hyperparameters such as the learning rate, regularization strength and network depth to the improve predictive accuracy.\n",
    "\n",
    "<input type=\"button\" class=\"w3-button w3-red\" value=\"Input Button\">\n",
    "<button height=\"50px\" width=\"150px\"><a href=\"https://observablehq.com/@yizhe-ang/interactive-visualization-of-linear-regression\">Visualization</a></button>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
